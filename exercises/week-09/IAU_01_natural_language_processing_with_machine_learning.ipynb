{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) with machine learning (ML)\n",
    "\n",
    "**Preprocessing of textual data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ayselaydin.medium.com/1-text-preprocessing-techniques-for-nlp-37544483c007\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:877/1*pt47uCcCn3X7XNYQg1dghw.png\" alt=\"Alt Text\" width=\"50%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing typically involves the following steps:\n",
    "\n",
    "- Lowercasing\n",
    "- Removing Punctuation & Special Characters\n",
    "- Stop-Words Removal\n",
    "- Removal of URLs\n",
    "- Removal of HTML Tags\n",
    "- Stemming & Lemmatization\n",
    "- Tokenization\n",
    "- Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK data - we need to do this only one time\n",
    "The download process can last longer (with GUI) and all data packages are bigger than 3.3 GB\n",
    "\n",
    "Uncomment the *nltk.download()* line to download all! It opens a new download window, which requires to click!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')   # Part-of-Speech Tagging (POS)\n",
    "# nltk.download('tagsets')\n",
    "# nltk.download('maxent_ne_chunker')            # Name Entity Recognition (NER)\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@abdallahashraf90x/tokenization-in-nlp-all-you-need-to-know-45c00cfa2df7\n",
    "\n",
    "Tokenization is one of preprocessing steps that breakes down the text into smaller units called tokens. These tokens can be words, characters, or even subwords, depending on the chosen tokenization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load data\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation)) \n",
    "\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in tokens]\n",
    "\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words] \n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF with TfidfVectorizer\n",
    "https://medium.com/@abhishekjainindore24/tf-idf-in-nlp-term-frequency-inverse-document-frequency-e05b65932f1d\n",
    "\n",
    "TF-IDF is a numerical statistic that helps identify the most meaningful words in a document. The idea behind TF-IDF is to quantify the importance of a term in a document with respect to its frequency in the document and its rarity across multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "dataset = [\n",
    "    \"I enjoy reading about Machine Learning and Machine Learning is my PhD subject\",\n",
    "    \"I would enjoy a walk in the park\",\n",
    "    \"I was reading in the library\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "tfIdf = vectorizer.fit_transform(dataset)\n",
    "\n",
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=vectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "\n",
    "print (df.head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF with TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "transformer = TfidfTransformer(use_idf=True)\n",
    "countVectorizer = CountVectorizer()\n",
    "\n",
    "wordCount = countVectorizer.fit_transform(dataset)\n",
    "newTfIdf = transformer.fit_transform(wordCount)\n",
    "\n",
    "df = pd.DataFrame(newTfIdf[0].T.todense(), index=countVectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "print (df.head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "URL: https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity\n",
    "\n",
    "The cosine similarity measures how similar two vectors (TF-IDF vectors) are. For this, the cosine of the angle between them is calculated.A value of 1 means the vectors are identical, 0 means they are completely different, and negative values indicate opposite directions.\n",
    "\n",
    "![Alt Text](https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# twenty dataset\n",
    "twenty = fetch_20newsgroups()\n",
    "tfidf = TfidfVectorizer().fit_transform(twenty.data)\n",
    "\n",
    "# cosine similarity\n",
    "cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
    "\n",
    "# top-5 related documents\n",
    "related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "print(related_docs_indices)\n",
    "print(cosine_similarities[related_docs_indices])\n",
    "\n",
    "# print the first result to check\n",
    "print(twenty.data[0])\n",
    "print(twenty.data[958])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification\n",
    "\n",
    "URL https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# twenty dataset\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "print(twenty_train.target_names)\n",
    "# print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Bag-of-words\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_test_counts = count_vect.transform(twenty_test.data)\n",
    "\n",
    "# TF-IDF\n",
    "transformer = TfidfTransformer()\n",
    "X_train_tfidf = transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = transformer.transform(X_test_counts)\n",
    "\n",
    "# Naive Bayes (NB) for text classification\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "\n",
    "# Performance of the model\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "The above code with Multinomial Naive Bayes can be written more elegantly with the scikit-learn pipeline. The code will be shorter and more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                    ])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# Performance of the model\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV with Naive Bayes\n",
    "We want and need to optimize the pipeline by hyper-parameter tuning. We may get some better classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "             }\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier\n",
    "We are trying another classifier called SGDClassifier instead of the previous Multinomial Naive Bayes. \n",
    "\n",
    "Let's see if this new classifier acts better in comparison with and without optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', \n",
    "                                                   penalty='l2',\n",
    "                                                   alpha=1e-3,  \n",
    "                                                   random_state=42)),\n",
    "                        ])\n",
    "text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# Performance of the model\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted_svm == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV with SVM\n",
    "\n",
    "Here might be more classifiers, e.g., SVM.\n",
    "\n",
    "We are going to try SVM with Grid Search optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf-svm__alpha': (1e-2, 1e-3),\n",
    "                 }\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming can improve classifier results too. Let's see if it works in our case example with Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), \n",
    "                             ('tfidf', TfidfTransformer()), \n",
    "                             ('mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)\n",
    "\n",
    "np.mean(predicted_mnb_stemmed == twenty_test.target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
