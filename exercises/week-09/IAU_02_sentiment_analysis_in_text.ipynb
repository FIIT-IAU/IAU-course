{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis in Text with NLTK\n",
    "URL https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "\n",
    "Sentiment analysis is the process of analyzing large volumes of text to determine whether it expresses a positive sentiment, a negative sentiment or a neutral sentiment.\n",
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "import re, string, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\('\n",
      "C:\\Temp\\ipykernel_10884\\823721929.py:9: SyntaxWarning: invalid escape sequence '\\('\n",
      "  token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n"
     ]
    }
   ],
   "source": [
    "# cleans tokens in tweets from noise\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    # assigns a part-of-speech tag to each one token using function pos_tag from NLTK\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # remove URLs\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        # Remove mentions, such as @username\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        # determine part of speech for lemmatization\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n' #noun\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v' #verb\n",
    "        else:\n",
    "            pos = 'a' #adjective\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer() #lemmatization reduces words to their base form\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        # add cleaned tokens to the list if they are not empty, punctuation, or stopwords\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "# creates a generator for all words from cleaned tokens\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "#  convert each token into a dictionary {word: True}\n",
    "def get_tweets_for_model(cleaned_tokens_list): \n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Data\n",
    "\n",
    "We load the example strings from the dataset \"positive_tweets\" and \"negatice_tweets\" to understand the structure of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print(positive_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data\n",
    "\n",
    "We remove stop words, tokenize the tweets into individual words, and prepare the data for further processing, such as cleaning tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "print(positive_tweet_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Noise\n",
    "We use our prepared function to remove noise from the tokenized tweets, creating cleaned lists of positive and negative tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "    \n",
    "print(positive_cleaned_tokens_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Word Density\n",
    "​\n",
    "Word density reflects the diversity of vocabulary in the dataset. A higher density indicates a more diverse vocabulary (less repetition). A lower density indicates more repetition of the same words.\n",
    "\n",
    "$$\n",
    "\\text{Word Density} = \\frac{\\text{Number of Unique Words}}{\\text{Total Number of Words}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Density in Positive Tweets: 0.2154\n",
      "Word Density in Negative Tweets: 0.2082\n"
     ]
    }
   ],
   "source": [
    "all_pos_words = list(get_all_words(positive_cleaned_tokens_list))\n",
    "all_neg_words = list(get_all_words(negative_cleaned_tokens_list))\n",
    "\n",
    "# Calculate total and unique word counts\n",
    "total_pos_words = len(all_pos_words)\n",
    "unique_pos_words = len(set(all_pos_words))\n",
    "\n",
    "total_neg_words = len(all_neg_words)\n",
    "unique_neg_words = len(set(all_neg_words))\n",
    "\n",
    "# Calculate word density\n",
    "positive_density = unique_pos_words / total_pos_words\n",
    "negative_density = unique_neg_words / total_neg_words\n",
    "\n",
    "print(f\"Word Density in Positive Tweets: {positive_density:.4f}\")\n",
    "print(f\"Word Density in Negative Tweets: {negative_density:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most popular words\n",
    "\n",
    "To count the most frequently occurring words, we can use FreqDist. It tells us the frequency distribution of each vocabulary item in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 332), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n",
      "({'#followfriday': True, 'top': True, 'engage': True, 'member': True, 'community': True, 'week': True, ':)': True}, 'Positive')\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                    for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                    for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "print(positive_dataset[0])\n",
    "print(len(positive_dataset))\n",
    "print(len(negative_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split dataset into the traning and testing sets. We will use NaiveBayesClassifier. The goal of this model is to learn the relationship between words and sentiment labels by calculating the likelihood of words in each class. For example, if the word “love” appears more often in positive texts, the model associates it with a positive tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9956666666666667\n",
      "Most Informative Features\n",
      "                      :) = True           Positi : Negati =   1648.1 : 1.0\n",
      "                follower = True           Positi : Negati =     41.3 : 1.0\n",
      "                     sad = True           Negati : Positi =     34.1 : 1.0\n",
      "                  arrive = True           Positi : Negati =     21.2 : 1.0\n",
      "                     bam = True           Positi : Negati =     20.8 : 1.0\n",
      "                    glad = True           Positi : Negati =     18.8 : 1.0\n",
      "                     x15 = True           Negati : Positi =     16.5 : 1.0\n",
      "               wonderful = True           Positi : Negati =     13.6 : 1.0\n",
      "                     idk = True           Negati : Positi =     13.1 : 1.0\n",
      "              definitely = True           Positi : Negati =     12.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dataset = positive_dataset + negative_dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the trained model to predict\n",
    "Now we can use our trained model to predict whether a sentence has a positive or negative tone, based on its words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ordered just once from TerribleCo, they screwed up, never used the app again.\n",
      "['i', 'order', 'just', 'once', 'from', 'terribleco', 'they', 'screw', 'up', 'never', 'use', 'the', 'app', 'again']\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "print(custom_tweet)\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "print(custom_tokens)\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
