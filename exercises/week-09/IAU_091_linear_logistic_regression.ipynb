{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear model\n",
    "\n",
    "## $R^2$ (coefficient of determination) score function\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n",
    "\n",
    "## $R^2 = 1 - \\frac{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\widehat{y_i})^2}{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y_i})^2} = 1 - \\frac{residual\\_sum\\_of\\_square}{total\\_sum\\_of\\_ squares} = 1 - \\frac{RSS}{TSS}$\n",
    "\n",
    "- $ùëÖ^2$ value of 100% means the model explains all the variation of the target variable. \n",
    "- And a value of 0% measures zero predictive power of the model. \n",
    "- **So, the higher the R-squared value, the better the model.**\n",
    "- During the worse cases, R2 score can even be negative. There are cases where the computational definition of $ùëÖ^2$ can yield negative values, depending on the definition used. $ùëÖ^2$ is bounded above by 1.0, but it is not bounded below. The reason is the evaluation (score) on unseen data, which can lead to results outside <0,1>. If $R^2$ on the same data fitted to the model will produce a score within <0, 1>, but **don't do that**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print(r2_score(y_true, y_pred))\n",
    "\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "print(r2_score(y_true, y_pred, multioutput='variance_weighted'))\n",
    "\n",
    "y_true = [1, 2, 3]\n",
    "y_pred = [1, 2, 3]\n",
    "print(r2_score(y_true, y_pred))\n",
    "\n",
    "y_true = [1, 2, 3]\n",
    "y_pred = [2, 2, 2]\n",
    "print(r2_score(y_true, y_pred))\n",
    "\n",
    "y_true = [1, 2, 3]\n",
    "y_pred = [3, 2, 1]\n",
    "print(r2_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes linear modeling, MSE and $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error (MSE)\n",
    "print('Mean squared error: %.2f' % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression\n",
    "\n",
    "## 2.1 Simple Linear Regression with scikit-learn\n",
    "\n",
    "URL https://realpython.com/linear-regression-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\n",
    "y = np.array([5, 20, 14, 32, 22, 38])\n",
    "\n",
    "# modelling\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "print('intercept:', model.intercept_)\n",
    "print('slope:', model.coef_)\n",
    "\n",
    "# scoring\n",
    "r_sq = model.score(x, y)\n",
    "print('coefficient of determination:', r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction on new data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.arange(5).reshape((-1, 1))\n",
    "print(x_new)\n",
    "\n",
    "y_new = model.predict(x_new)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**two-dimentional y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LinearRegression().fit(x, y.reshape((-1, 1)))\n",
    "print('intercept:', model2.intercept_)\n",
    "print('slope:', model2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multiple Linear Regression with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# modelling\n",
    "model = LinearRegression().fit(x, y)\n",
    "print('intercept:', model.intercept_)\n",
    "print('slope:', model.coef_)\n",
    "\n",
    "# scoring\n",
    "r_sq = model.score(x, y)\n",
    "print('coefficient of determination:', r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction on new data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.arange(10).reshape((-1, 2))\n",
    "print(x_new)\n",
    "\n",
    "y_new = model.predict(x_new)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Polynomial Regression with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\n",
    "y = np.array([15, 11, 2, 8, 25, 32])\n",
    "print(x)\n",
    "\n",
    "# Transformer model 2. degree\n",
    "# transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# transformer.fit(x)\n",
    "# x_ = transformer.transform(x)\n",
    "\n",
    "# the same code effect, but shorter\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n",
    "print(x_)\n",
    "\n",
    "# modelling\n",
    "model = LinearRegression().fit(x_, y)\n",
    "print('intercept:', model.intercept_)\n",
    "print('coefficients:', model.coef_)\n",
    "\n",
    "# Scoring\n",
    "r_sq = model.score(x_, y)\n",
    "print('coefficient of determination:', r_sq)\n",
    "\n",
    "# prediction\n",
    "y_pred = model.predict(x_)\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Mathieu Blondel\n",
    "#         Jake Vanderplas\n",
    "# License: BSD 3 clause\n",
    "# URL https://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "# function to approximate by polynomial interpolation\n",
    "def f(x):\n",
    "    return x * np.sin(x)\n",
    "\n",
    "\n",
    "# generate points used to plot\n",
    "x_plot = np.linspace(0, 10, 100)\n",
    "\n",
    "# generate points and keep a subset of them\n",
    "x = np.linspace(0, 10, 100)\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(x)\n",
    "x = np.sort(x[:20])\n",
    "y = f(x)\n",
    "\n",
    "# create matrix versions of these arrays\n",
    "X = x[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 2\n",
    "plt.plot(x_plot, \n",
    "         f(x_plot), \n",
    "         color='cornflowerblue', \n",
    "         linewidth=lw,\n",
    "         label=\"ground truth\")\n",
    "plt.scatter(x, y, \n",
    "            color='navy', \n",
    "            s=30, \n",
    "            marker='o', \n",
    "            label=\"training points\")\n",
    "\n",
    "for count, degree in enumerate([3, 4, 5]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), \n",
    "                          Ridge())\n",
    "    model.fit(X, y)\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, \n",
    "             y_plot, \n",
    "             color=colors[count], \n",
    "             linewidth=lw,\n",
    "             label=\"degree %d\" % degree)\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Linear Regression with statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "x = sm.add_constant(x)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# modelling\n",
    "model = sm.OLS(y, x)\n",
    "\n",
    "# scoring\n",
    "results = model.fit()\n",
    "print('coefficient of determination:', results.rsquared)\n",
    "print('adjusted coefficient of determination:', results.rsquared_adj)\n",
    "print('regression coefficients:', results.params)\n",
    "\n",
    "# predict\n",
    "# print('predicted response:', results.fittedvalues, sep='\\n')\n",
    "# print('predicted response:', results.predict(x), sep='\\n')\n",
    "\n",
    "# predict on new data\n",
    "x_new = sm.add_constant(np.arange(10).reshape((-1, 2)))\n",
    "print(x_new)\n",
    "y_new = results.predict(x_new)\n",
    "print(y_new)\n",
    "\n",
    "# summary\n",
    "# print(results.summary())\n",
    "# print(results.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression\n",
    "\n",
    "URL https://realpython.com/logistic-regression-python/#classification\n",
    "\n",
    "<!--\n",
    "### $\\hat{y}^{(i)}=\\beta_{0}+\\beta_{1}x^{(i)}_{1}+\\ldots+\\beta_{p}x^{(i)}_{p}$\n",
    "\n",
    "### $ P(y^{(i)}=1)=\\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1}x^{(i)}_{1}+\\ldots+\\beta_{p}x^{(i)}_{p})}} $\n",
    "\n",
    "### $ ùëù(ùê±) = \\frac{1}{1 + e^{‚àíùëì(ùê±)}} $\n",
    "\n",
    "### $ ùëì(ùê±) = log \\left( \\frac{ùëù(ùê±)}{1 ‚àí ùëù(ùê±)} \\right) $\n",
    "//-->\n",
    "\n",
    "## 3.1 scikit-learn: Logistic Regression\n",
    "LogisticRegression(**C=1.0**, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
    "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "- 'liblinear' solver doesn‚Äôt work without regularization.\n",
    "- 'newton-cg', 'sag', 'saga', and 'lbfgs' don‚Äôt support L1 regularization.\n",
    "- 'saga' is the only solver that supports elastic-net regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y, y_pred):\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(conf_m)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    return\n",
    "\n",
    "def eval_model(model, x, y):\n",
    "    p_pred = model.predict_proba(x)\n",
    "    y_pred = model.predict(x)\n",
    "    score_ = model.score(x, y)\n",
    "    report = classification_report(y, y_pred)\n",
    "    print(p_pred, '\\n', y_pred, '\\n', score_, '\\n', report)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "x = np.arange(10).reshape(-1, 1)\n",
    "print(x)\n",
    "\n",
    "# y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "y = np.array([0, 1, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and train it\n",
    "model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "model.fit(x, y)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_model(model, x, y)\n",
    "# plot_confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning: set C=10.0 for better prediction? default C=1.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and train it\n",
    "model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\n",
    "model.fit(x, y)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_model(model, x, y)\n",
    "# plot_confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 StatsModels: Logistic Regression¬∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get data\n",
    "x = np.arange(10).reshape(-1, 1)\n",
    "y = np.array([0, 1, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "# Create a model and train it\n",
    "model = sm.Logit(y, x)\n",
    "result = model.fit(method='newton')\n",
    "\n",
    "# Evaluate the model\n",
    "result.predict(x)\n",
    "print(result.pred_table())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report with StatsModels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.summary()\n",
    "# result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report with scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification + report with scikit-learn\n",
    "y_pred = (result.predict(x) >= 0.5).astype(int)\n",
    "report = classification_report(y, y_pred)\n",
    "print(report) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 AUC curve For Binary Classification with Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44)\n",
    "clf = LogisticRegression(penalty='l2', \n",
    "                         C=0.1, \n",
    "                         max_iter=5000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy', metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(pd.DataFrame(cm, \n",
    "                   columns=['Predicted Benign', 'Predicted Malignant'], \n",
    "                   index=['Actual Benign', 'Actual Malignant']), '\\n')\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print('True Positives: {tp}', \n",
    "      'False Positives: {fp}', \n",
    "      'True Negatives: {tn}',\n",
    "      'False Negatives: {fn}', '\\n')\n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "print('FPR', fpr, \n",
    "      '\\nTPR', tpr, \n",
    "      '\\nthreshold', \n",
    "      '\\nthreshold',  \n",
    "      '\\nROC-AUC', auc)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc) )\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multiclass classification\n",
    "\n",
    "## 4.1 One-vs-Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "X = np.array([\n",
    "    [10, 10],\n",
    "    [8, 10],\n",
    "    [-5, 5.5],\n",
    "    [-5.4, 5.5],\n",
    "    [-20, -20],\n",
    "    [-15, -20]\n",
    "])\n",
    "y = np.array([0, 0, 1, 1, 2, 2])\n",
    "\n",
    "clf = OneVsRestClassifier(SVC()).fit(X, y)\n",
    "clf.predict([[-19, -20], [9, 9], [-5, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 One-vs-One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.33, \n",
    "    shuffle=True, \n",
    "    random_state=0)\n",
    "\n",
    "clf = OneVsOneClassifier(\n",
    "    LinearSVC(max_iter=10000, random_state=0)).fit(X_train, y_train)\n",
    "\n",
    "clf.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
