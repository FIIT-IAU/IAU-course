{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification metrics\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 0, 0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [0, 0, 0, 0, 0, 1, 0, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Accuracy, precission, recall\n",
    "\n",
    "### $Accuracy = \\frac{TP+TN}{TP+FP+FN+TN}$\n",
    "\n",
    "### $Precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "### $Recall = \\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "print(accuracy_score(y_true, y_pred, normalize=False))\n",
    "\n",
    "print(precision_score(y_true, y_pred, average='macro'))\n",
    "print(precision_score(y_true, y_pred, average='micro'))\n",
    "print(precision_score(y_true, y_pred, average='weighted'))\n",
    "print(precision_score(y_true, y_pred, average=None))\n",
    "\n",
    "print(recall_score(y_true, y_pred, average='macro'))\n",
    "print(recall_score(y_true, y_pred, average='micro'))\n",
    "print(recall_score(y_true, y_pred, average='weighted'))\n",
    "print(recall_score(y_true, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 F1-score\n",
    "\n",
    "### $F1 = 2\\frac{precission \\cdot recall}{precission + recall}$\n",
    "\n",
    "### $F_β = (1 + β^2)\\frac{precission \\cdot recall}{β^2 \\cdot precission + recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "print(f1_score(y_true, y_pred))\n",
    "print(fbeta_score(y_true, y_pred, beta=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = [2, 0, 2, 2, 0, 1]\n",
    "# y_pred = [0, 0, 2, 2, 0, 2]\n",
    "\n",
    "# y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "# y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_true, y_pred)\n",
    "# confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp = disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 ROC and AUC\n",
    "\n",
    "### $TPR = Sensivity = Recall = \\frac{TP}{TP+FN}$ \n",
    "\n",
    "### $FPR = \\frac{FP}{FP+TN}$\n",
    "\n",
    "alternatively:\n",
    "\n",
    "### $FPR = 1 - Specificity = 1 - \\frac{TN}{TN+FP}$\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ea/Python_roc_curve.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# y_true = np.array([0, 0, 1, 1])\n",
    "# y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "y_scores = y_pred\n",
    "roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Log loss, aka logistic loss or cross-entropy loss\n",
    "\n",
    "### $-log P(\\frac{y_t}{y_p}) = -(y_t log(y_p) + (1 - y_t) log(1 - y_p))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],\n",
    "         [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regression metrics\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Mean Square Error (MSE) and Root Mean Square Error (RMSE)\n",
    "\n",
    "### $MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\widehat{y_i})^2$\n",
    "\n",
    "### $RMSE = \\sqrt{MSE}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "print(mean_squared_error(y_true, y_pred))                  # RMSE\n",
    "print(mean_squared_error(y_true, y_pred, squared=False))   # MSE\n",
    "\n",
    "y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
    "y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
    "\n",
    "print(mean_squared_error(y_true, y_pred, multioutput='raw_values'))\n",
    "print(mean_squared_error(y_true, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Mean Absolute Error (MAE)\n",
    "\n",
    "### $MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\widehat{y_i}|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print(mean_absolute_error(y_true, y_pred))\n",
    "\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "print(mean_absolute_error(y_true, y_pred))\n",
    "print(mean_absolute_error(y_true, y_pred, multioutput='raw_values'))\n",
    "print(mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 $R^2$ or R-squared  ot Coefficient of determination\n",
    "\n",
    "### $R^2 = 1 - \\frac{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\widehat{y_i})^2}{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y_i})^2}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "print(r2_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Mean Absolute Percentage Error (MAPE) \n",
    "\n",
    "### $MAPE ={\\frac {100\\%}{n}}\\sum_{i=1}^{n}\\left|{\\frac {y_{i} - \\widehat{y_i}}{y_{i}}}\\right|$\n",
    "\n",
    "## 2.5 Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "### $SMAPE = \\frac{100\\%}{n} \\sum_{i=1}^n \\frac{|y_i - \\widehat{y_i} | }{ \\frac{1}{2} (|y_i| + |\\widehat{y_i}|)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: these metrics are not in many libraries because of (their) division by zero. \n",
    "# Their use is according to the need and their implementation is quite trivial.\n",
    "\n",
    "# Your code: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Micro and Macro-averaging\n",
    "\n",
    "### Micro-averaging\n",
    "\n",
    "### $precision = \\frac{\\sum_{i=1}^{|C|}TP_i}{\\sum_{i=1}^{|C|}(TP_i + FP_i)}$ ; $recall = \\frac{\\sum_{i=1}^{|C|}TP_i}{\\sum_{i=1}^{|C|}(TP_i + FN_i)}$\n",
    "\n",
    "### Macro-averaging\n",
    "\n",
    "### $precision = \\frac{\\sum_{i=1}^{|C|}precission_{C_i}}{|C|}$ ; $recall = \\frac{\\sum_{i=1}^{|C|}recall_{C_i}}{|C|}$\n",
    "\n",
    "Where $|C|$ is number of classes\n",
    "\n",
    "### Weighted-averaging\n",
    "\n",
    "<img src=\"img/weighted.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "\n",
    "y_pred = [1, 1, 0]\n",
    "y_true = [1, 1, 1]\n",
    "print(classification_report(y_true, y_pred, labels=[1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Similarity metrics\n",
    "\n",
    "- Cosine similarity\n",
    "- Euclidean distance\n",
    "- Manhattan distance\n",
    "- Levenshtein distance\n",
    "\n",
    "## 3.1 Cosine similarity\n",
    "\n",
    "### $cosine = \\frac{y \\cdot \\widehat{y}}{\\|y \\|  \\cdot  \\|\\widehat{y} \\|} = \\frac{\\sum_{i=1}^{n}{y_i  \\widehat{y_i} } }  \n",
    "\t\t\t\t { {\\sqrt {\\sum_{i=1}^{n}{y_i^{2} } } }  {\\sqrt {\\sum_{i=1}^{n}{\\widehat{y_i}^{2} } } } }$\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
    "\n",
    "and many other implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "a = [3, 45, 7, 2]\n",
    "b = [2, 54, 13, 15]\n",
    "\n",
    "cos_sim = 1 - spatial.distance.cosine(a, b)\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(A,B): \n",
    "    return (sum(a*b for a,b in zip(A,B)))\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a,b) / ( (dot(a,a) **.5) * (dot(b,b) ** .5) )\n",
    "\n",
    "cosine_similarity(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "a1 = np.array(a).reshape(1, -1)\n",
    "b1 = np.array(b).reshape(1, -1)\n",
    "print(a1)\n",
    "print(b1)\n",
    "\n",
    "cos_sim = cosine_similarity(a1, b1)\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Euclidean distance\n",
    "\n",
    "### $d(p,q) = d(q,p) = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \\cdots + (q_n-p_n)^2} = \\sqrt{\\sum_{i=1}^n(q_i-p_i)^2}$\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "X = [[0, 1], [1, 1]]\n",
    "print(X)\n",
    "\n",
    "# distance between rows of X\n",
    "euclidean_distances(X, X)\n",
    "\n",
    "# get distance to origin\n",
    "euclidean_distances(X, [[0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Manhattan distance (or taxicab distance $d_1$)\n",
    "\n",
    "### $d_1(p, q)= \\|p-q\\|_1 = \\sum_{i=1}^n|p_i-q_i|$\n",
    "\n",
    "where $p, q$ are vectors; $p=(p_1, p_2,\\dots, p_n)$, $q=(q_1, q_2,\\dots, q_n)$\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.manhattan_distances.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "manhattan_distances([[3]], [[3]])\n",
    "\n",
    "manhattan_distances([[3]], [[2]])\n",
    "\n",
    "manhattan_distances([[2]], [[3]])\n",
    "\n",
    "manhattan_distances([[1, 2], [3, 4]],         [[1, 2], [0, 3]])\n",
    "\n",
    "X = np.ones((1, 2))\n",
    "y = np.full((2, 2), 2.)\n",
    "manhattan_distances(X, y, sum_over_features=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Levenshtein distance\n",
    "Levenshtein distance between two strings $a,b$ is \n",
    "\n",
    "$\n",
    "    \\label{eq:lev}\n",
    "    lev_{a,b}(i,j)=\n",
    "      \\begin{cases}\n",
    "      \\max(i,j) & \\text{if $\\min(i,j)=0$,} \\\\[1ex]\n",
    "      \\begin{aligned}[b]\n",
    "      \\min\\bigl(lev_{a,b}&(i-1,j)+1, \\\\\n",
    "                lev_{a,b}&(i,j-1)+1, \\\\\n",
    "                lev_{a,b}&(i-1,j-1)+1_{(a_i\\ne b_j)}\n",
    "          \\bigr)\n",
    "      \\end{aligned} & \\text{otherwise.}\n",
    "    \\end{cases}\n",
    "$\n",
    "\n",
    "where\n",
    "    $i$ is the terminal character position of $a$,\n",
    "    $j$ is the terminal character position of $b$, \n",
    "    and positions are 1-indexed.\n",
    "    \n",
    "<!-- <img src=\"img/levenshtein.png\" /> //-->\n",
    "\n",
    "URL https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n",
    "\n",
    "URL https://medium.com/@ethannam/understanding-the-levenshtein-distance-equation-for-beginners-c4285a5604f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
