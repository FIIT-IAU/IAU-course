{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spracovanie textu\n",
    "\n",
    "URL https://github.com/FIIT-IAU\n",
    "\n",
    "\n",
    "## Extrakcia čŕt z textu\n",
    "\n",
    "Aby sme mohli text klasifikovať, hľadať zhluky podobných dokumentov a pod.\n",
    "\n",
    "#### Príklad: chceme rozlíšiť, kto je autorom textu\n",
    "\n",
    "Edgar Allan Poe vs. Mary Shelley vs. HP Lovecraft: https://www.kaggle.com/c/spooky-author-identification\n",
    "\n",
    "**Aké črty (features) by som mohol extrahovať z viet?**\n",
    "* Dĺžka vety\n",
    "* Počet slov vo vete\n",
    "* Priemerná dĺžka slov vo vete\n",
    "* Zložitosť vety (tzv. text readability metrics, napr. Flesh-Kincaid)\n",
    "* Počet spojok/predložiek/iných slovných druhov\n",
    "* **Frekvencia použitých slov - prevod vety (textu) do vektorovej reprezentácie**\n",
    "\n",
    "#### Vo všeobecnosti\n",
    "\n",
    "* Segmentácia textu \n",
    "* Prevod textu do vektorovej reprezentácie (tzv. *bag of words*)\n",
    "* Identifikácia kľúčových slov, resp. často sa spolu vyskytujúcich slov (tokenov)\n",
    "* Určovanie podobnosti dvoch textových dokumentov\n",
    "\n",
    "## Metódy na spracovanie textu\n",
    "- Regulárne výrazy, konečné automaty, bezkontextové gramatiky\n",
    "- Pravidlové, slovníkové prístupy\n",
    "- Prístupy strojového učenia (Markovovské modely, **hlboké neurónové siete**)\n",
    "\n",
    "\n",
    "#### Väčšina metód je jazykovo-závislá\n",
    "- Veľa dostupných modelov pre angličtinu, nemčinu, španielčinu, ...\n",
    "\n",
    "\n",
    "#### Reprezentácia textu\n",
    "- Textový dokument väčšinou reprezentujeme pomocou množiny slov (angl. *bag-of-words*) = **vektorom**.\n",
    "- Zložky vektora predstavujú jednotlivé slová, resp. n-gramy zo slovníka (pre celý korpus/jazyk).\n",
    "\n",
    "Hodnotou zložiek vektora môže byť:\n",
    "* výskyt (binárne)\n",
    "* početnosť\n",
    "* frekvencia\n",
    "* váhovaná frekvencia\n",
    "\n",
    "#### Prevod textu na vektor\n",
    "\n",
    "1. Tokenizácia (rozdelenie textu na vety, následne na slová)\n",
    "2. Normalizácia textu\n",
    "   - prevod na malé písmená\n",
    "   - stemming alebo lematizácia\n",
    "   - odstránenie stopslov (spojky, predložky a pod.)\n",
    "3. Vytvorenie slovníka\n",
    "4. Vytvorenie vektora - zložky slová zo slovníka; väčšinou riedky (angl. *sparse*; veľa núl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenizácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "text = \"\"\"At eight o'clock on Thursday morning \n",
    "... Arthur didn't feel very good. He closed his eyes and went to bed again.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sent = sentences[0]\n",
    "\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normalizácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tokens = [token.lower() for token in tokens if token not in \".,?!...\"]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming alebo lematizácia?\n",
    "\n",
    "- Stemming vráti koreň slova. Napr. *ryba -> ryb*\n",
    "- Lematizácia prevádza slová na ich základný slovníkový tvar. Napr. *rybe -> ryba*\n",
    "- **Je to vždy jedno alebo druhé.** Prevod na koreň slova sa používa viac pri málo flexných jazykoch (napr. angličtina). Pri flexných jazykoch (napr. slovenčina) je preferovaná lematizácia.\n",
    "- **Stemming** - pre angličtinu napr. [Porterov algoritmus (1980)](https://www.cs.odu.edu/~jbollen/IR04/readings/readings5.pdf)\n",
    "- **Lematizácia** - väčšinou slovníkové metódy (morfologická databáza, resp. tvaroslovník); pre slovenčinu: https://korpus.sk/morphology_database.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "stemmed = [porter.stem(token) for token in tokens]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "lemmatized = [wnl.lemmatize(token) for token in tokens]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Odstránenie stopslov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "normalized_tokens = [token for token in stemmed if token not in stopwords]\n",
    "print(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prevod na vektorovú reprezentáciu\n",
    "\n",
    "Datasetom [20 newsgroups](http://qwone.com/~jason/20Newsgroups/):\n",
    "\n",
    "*\"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [token.lower() for token in tokens if token.isalpha() and token.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_docs = [preprocess_text(text) for text in twenty_train.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(tokenized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "print(corpus[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TF-IDF = term frequency * inverse document frequency\n",
    "\n",
    "`TF` – frekvencia slova v aktuálnom dokumente\n",
    "\n",
    "`IDF` – záporný logaritmus pravdepodobnosti výskytu slova v dokumentoch korpusu (rovnaká pre všetky dokumenty)\n",
    "\n",
    "### $ tf(t,d)=\\frac{f_{t,d}}{\\sum_{t' \\in d}{f_{t',d}}} $\n",
    "\n",
    "### $ idf(t,D) = -\\log_2{\\frac{|{d \\in D: t \\in d}|}{N}} = \\log_2{\\frac{N}{|{d \\in D: t \\in d}|}} $\n",
    "\n",
    "Rôzne varianty (váhovacie schémy): https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_model = models.TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "tfidf_corpus[10][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Podobnosť vektorov\n",
    "\n",
    "Podobnosť pomocou Euklidovskej vzdialenosti\n",
    "\n",
    "### $ sim(u,v) = 1- d(u,v) = 1 - \\sqrt{\\sum_{i=1}^{n}{(v_i-u_i)^2}} $\n",
    "\n",
    "Kosínusová podobnosť\n",
    "\n",
    "### $sim(u,v) = cos(u,v) = \\frac{u \\cdot v}{||u||||v||} =\\frac{\\sum_{i=1}^{n}{u_iv_i}}{\\sum_{i=1}^{n}{u_i}\\sum_{i=1}^{n}{v_i}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(tfidf_corpus)\n",
    "index[tfidf_corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extrakcia čŕt pomocou scikit-learn\n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(count_vect.vocabulary_.get(u'algorithm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Natrénujeme klasifikátor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sprehľadnenie a automatizácia predspracovania: Pipelines\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_ppl = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text_ppl.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "[twenty_train.target_names[cat] for cat in text_ppl.predict(docs_new)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vlastný transformátor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class MyTransformer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Iné časté úlohy (pred)spracovania textu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rozpoznávanie slovných druhov = Part-of-Speech Tagging (POS)\n",
    "\n",
    "Slovný druh, číslo, čas, prípadne ďalšie gramatické kategórie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('tagsets')\n",
    "\n",
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rozpoznávanie menných entít = Name Entity Recognition (NER)\n",
    "\n",
    "Osoby, organizácie, miesta a pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(entities.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## N-gramy\n",
    "\n",
    "Vo všobecnosti ide o postupnosť $N$ po sebe idúcich položiek. V texte väčšinou na úrovni slov.\n",
    "- bigramy\n",
    "- trigramy\n",
    "- skipgramy - $k$-skip-$n$-gramy\n",
    "- https://books.google.com/ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sent)\n",
    "bigrams = list(nltk.bigrams(tokens))\n",
    "print(bigrams[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dá sa nastaviť aj v `CountVectorizer` transformátore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WordNet\n",
    "\n",
    "* Lexikálna databáza\n",
    "* Organizovaná pomocou tzv. synsetov (množín synoným)\n",
    "  * Podstatné mená, slovesá, prídavné mená, príslovky\n",
    "* Prepojenia medzi synsetmi\n",
    "  * Antonymá, hyperonymá, hyponymá, holonymá, meronymá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(wn.synsets('car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "car = wn.synset('car.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "car.lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "car.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "car.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(car.hyponyms()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "car.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(car.part_meronyms()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wn.synsets('black')[0].lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vektorová reprezentácia slov - word2vec\n",
    "\n",
    "Každé slovo má naučený vektor reálnych čísel, ktoré reprezentujú rôzne jeho vlastnosti a zachytávajú viaceré lingvistické pravidelnosti. Môžeme počítať podobnosť medzi slovami ako podobnosť dvoch vektorov.\n",
    "\n",
    "vector('Paris') - vector('France') + vector('Italy') ~= vector('Rome')\n",
    "\n",
    "vector('king') - vector('man') + vector('woman') ~= vector('queen')\n",
    "\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html\n",
    "- https://medium.com/@mishra.thedeepak/word2vec-in-minutes-gensim-nlp-python-6940f4e00980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "\n",
    "sentences = brown.sents()\n",
    "model = models.Word2Vec(sentences, min_count=1)\n",
    "model.save('brown_model')\n",
    "model = models.Word2Vec.load('brown_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print(model.most_similar(\"mother\"))\n",
    "print(model.wv.most_similar(\"mother\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "print(model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Užitočné slovníky\n",
    "- ConceptNet: http://conceptnet.io/\n",
    "- Sentiment a emócie: [WordNet-Affect](http://wndomains.fbk.eu/wnaffect.html), \n",
    "- [SenticNet](https://sentic.net/), \n",
    "- [EmoSenticNet](https://www.gelbukh.com/emosenticnet/)\n",
    "\n",
    "\n",
    "# Nástroje na spracovanie textu v Pythone\n",
    "\n",
    "- [NLTK](http://www.nltk.org/)\n",
    "- [Gensim](https://radimrehurek.com/gensim/tutorial.html)\n",
    "- [sklearn.feature_extraction.text](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "#### Nástroje (mimo Pythonu)\n",
    "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) - rozhranie aj cez NLTK\n",
    "- [Apache OpenNLP](https://opennlp.apache.org/)\n",
    "- [WordNet](https://wordnet.princeton.edu/) - rozhranie cez NLTK\n",
    "\n",
    "\n",
    "# Extrakcia čŕt sa robí aj s inými typmi vstupov\n",
    "- Obrázky (sklearn.feature_extraction.image, [scikit-image](https://scikit-image.org/))\n",
    "- Videá ([scikit-video](http://www.scikit-video.org/stable/))\n",
    "- Signál, napr. zvuk ([scikit-signal](https://docs.scipy.org/doc/scipy/reference/signal.html), [scikit-sound](http://work.thaslwanter.at/sksound/html/))\n",
    "\n",
    "\n",
    "# Ďalšie lingvistické modely\n",
    "- [fastText](https://fasttext.cc/), [ELMo](https://allennlp.org/elmo), [BERT](https://github.com/google-research/bert), [GloVe](https://nlp.stanford.edu/projects/glove/): nejaké základné porovnanie [tu](https://www.quora.com/What-are-the-main-differences-between-the-word-embeddings-of-ELMo-BERT-Word2vec-and-GloVe)\n",
    "- [sentence embeddings](https://github.com/oborchers/Fast_Sentence_Embeddings)\n",
    "- [doc2vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "- ...a ďalšie\n",
    "\n",
    "# Pre slovenčinu\n",
    "\n",
    "- [NLP4SK](http://arl6.library.sk/nlp4sk/)\n",
    "- [Slovenský národný korpus](https://korpus.sk/)\n",
    "- [word2vec](https://github.com/essential-data/word2vec-sk)\n",
    "- a [ďalšie...](https://github.com/essential-data/nlp-sk-interesting-links)\n",
    "\n",
    "\n",
    "# Zdroje\n",
    "- [Dan Jurafsky, James H. Martin: Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "- http://www.nltk.org/book/\n",
    "- https://radimrehurek.com/gensim/tutorial.html\n",
    "- https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
