{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression\n",
    "\n",
    "URL https://realpython.com/logistic-regression-python/#classification\n",
    "\n",
    "<!--\n",
    "### $\\hat{y}^{(i)}=\\beta_{0}+\\beta_{1}x^{(i)}_{1}+\\ldots+\\beta_{p}x^{( i)}_{p}$\n",
    "\n",
    "### $ P(y^{(i)}=1)=\\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1}x^{(i)}_{ 1}+\\ldots+\\beta_{p}x^{(i)}_{p})}} $\n",
    "\n",
    "### $ ð‘(ð±) = \\frac{1}{1 + e^{âˆ’ð‘“(ð±)}} $\n",
    "\n",
    "### $ ð‘“(ð±) = log \\left( \\frac{ð‘(ð±)}{1 âˆ’ ð‘(ð±)} \\right) $\n",
    "//-->\n",
    "\n",
    "## 1.1 scikit-learn: Logistic Regression\n",
    "LogisticRegression(**C=1.0**, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
    "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "- 'liblinear' solver doesn't work without regularization.\n",
    "- 'newton-cg', 'sag', 'saga', and 'lbfgs' don't support L1 regularization.\n",
    "- 'saga' is the only solver that supports elastic-net regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y, y_pred):\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(conf_m)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    return\n",
    "\n",
    "def eval_model(model, x, y):\n",
    "    p_pred = model.predict_proba(x)\n",
    "    y_pred = model.predict(x)\n",
    "    score_ = model.score(x, y)\n",
    "    report = classification_report(y, y_pred)\n",
    "    print(p_pred, '\\n', y_pred, '\\n', score_, '\\n', report)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "x = np.arange(10).reshape(-1, 1)\n",
    "print(x)\n",
    "\n",
    "# y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "y = np.array([0, 1, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and train it\n",
    "model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "model.fit(x, y)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_model(model, x, y)\n",
    "# plot_confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning: set C=10.0 for better prediction? default C=1.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and train it\n",
    "model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\n",
    "model.fit(x, y)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_model(model, x, y)\n",
    "# plot_confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 StatsModels: Logistic RegressionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get data\n",
    "x = np.arange(10).reshape(-1, 1)\n",
    "y = np.array([0, 1, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "# Create a model and train it\n",
    "model = sm.Logit(y, x)\n",
    "result = model.fit(method='newton')\n",
    "\n",
    "# Evaluate the model\n",
    "result.predict(x)\n",
    "print(result.pred_table())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report with StatsModels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.summary()\n",
    "# result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report with scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification + report with scikit-learn\n",
    "y_pred = (result.predict(x) >= 0.5).astype(int)\n",
    "report = classification_report(y, y_pred)\n",
    "print(report) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 AUC curve For Binary Classification with Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44)\n",
    "clf = LogisticRegression(penalty='l2', \n",
    "                         C=0.1, \n",
    "                         max_iter=5000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy', metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(pd.DataFrame(cm, \n",
    "                   columns=['Predicted Benign', 'Predicted Malignant'], \n",
    "                   index=['Actual Benign', 'Actual Malignant']), '\\n')\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print('True Positives: ', tp, \n",
    "      'False Positives: ', fp, \n",
    "      'True Negatives: ', tn,\n",
    "      'False Negatives: ', fn, '\\n')\n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "print('FPR', fpr, '\\n\\n', \n",
    "      'TPR', tpr, '\\n\\n',\n",
    "      'threshold', threshold, '\\n\\n', \n",
    "      'ROC-AUC', auc, '\\n\\n')\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc) )\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multiclass classification\n",
    "\n",
    "## 2.1 One-vs-Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "X = np.array([\n",
    "    [10, 10],\n",
    "    [8, 10],\n",
    "    [-5, 5.5],\n",
    "    [-5.4, 5.5],\n",
    "    [-20, -20],\n",
    "    [-15, -20]\n",
    "])\n",
    "y = np.array([0, 0, 1, 1, 2, 2])\n",
    "\n",
    "clf = OneVsRestClassifier(SVC()).fit(X, y)\n",
    "clf.predict([[-19, -20], [9, 9], [-5, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 One-vs-One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.33, \n",
    "    shuffle=True, \n",
    "    random_state=0)\n",
    "\n",
    "clf = OneVsOneClassifier(\n",
    "    LinearSVC(max_iter=10000, random_state=0)).fit(X_train, y_train)\n",
    "\n",
    "clf.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes: Feature selection - Embedded\n",
    "\n",
    "Combine the advantages of filters and wrappers\n",
    "- The model that is being trained will directly choose the attributes that are best for it\n",
    "\n",
    "Few models support it\n",
    "* Linear models penalized by L1 (Lasso) or L1+L2 (Elastic Net) regularization: SVM, Linear regression, Logistic regression ...\n",
    "\n",
    "- Regularization introduces into the model a penalty for the number / size of model attribute weights. It's not just a prediction error. Naturally, a simpler model is chosen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
