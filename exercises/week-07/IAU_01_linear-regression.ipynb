{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear model\n",
    "\n",
    "## $R^2$ (coefficient of determination) score function\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n",
    "\n",
    "## $R^2 = 1 - \\frac{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\widehat{y_i})^2}{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y_i})^2} = 1 - \\frac{residual\\_sum\\_of\\_square}{total\\_sum\\_of\\_ squares} = 1 - \\frac{RSS}{TSS}$\n",
    "\n",
    "- $ùëÖ^2$ value of 100% means the model explains all the variation of the target variable. \n",
    "- And a value of 0% measures zero predictive power of the model. \n",
    "- **So, the higher the R-squared value, the better the model.**\n",
    "- During the worst cases, R2 score can even be negative. There are cases where the computational definition of $ùëÖ^2$ can yield negative values, depending on the definition used. $ùëÖ^2$ is bounded above by 1.0, but it is not bounded below. The reason is the evaluation (score) on unseen data, which can lead to results outside <0.1>. If $R^2$ on the same data fitted to the model will produce a score within <0, 1>, but **don't do that**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print(r2_score(y_true, y_pred))\n",
    "\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "print(r2_score(y_true, y_pred, multioutput='variance_weighted'))\n",
    "\n",
    "y_true = [1, 2, 3]\n",
    "y_pred = [1, 2, 3]\n",
    "print(r2_score(y_true, y_pred))\n",
    "\n",
    "y_true = [1, 2, 3]\n",
    "y_pred = [2, 2, 2]\n",
    "print(r2_score(y_true, y_pred))\n",
    "\n",
    "y_true = [1, 2, 3]\n",
    "y_pred = [3, 2, 1]\n",
    "print(r2_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes linear modeling, MSE and $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error (MSE)\n",
    "print('Mean squared error: %.2f' % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression\n",
    "\n",
    "## 2.1 Simple Linear Regression with scikit-learn\n",
    "\n",
    "URL https://realpython.com/linear-regression-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\n",
    "y = np.array([5, 20, 14, 32, 22, 38])\n",
    "\n",
    "# modeling\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "print('intercept:', model.intercept_)\n",
    "print('slope:', model.coef_)\n",
    "\n",
    "# scoring\n",
    "r_sq = model.score(x, y)\n",
    "print('coefficient of determination:', r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction on new data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.arange(5).reshape((-1, 1))\n",
    "print(x_new)\n",
    "\n",
    "y_new = model.predict(x_new)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**two-dimensional y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LinearRegression().fit(x, y.reshape((-1, 1)))\n",
    "print('intercept:', model2.intercept_)\n",
    "print('slope:', model2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multiple Linear Regression with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# modeling\n",
    "model = LinearRegression().fit(x, y)\n",
    "print('intercept:', model.intercept_)\n",
    "print('slope:', model.coef_)\n",
    "\n",
    "# scoring\n",
    "r_sq = model.score(x, y)\n",
    "print('coefficient of determination:', r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction on new data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.arange(10).reshape((-1, 2))\n",
    "print(x_new)\n",
    "\n",
    "y_new = model.predict(x_new)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Polynomial Regression with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\n",
    "y = np.array([15, 11, 2, 8, 25, 32])\n",
    "print(x)\n",
    "\n",
    "# Transformer model 2nd degree\n",
    "# transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# transformer.fit(x)\n",
    "# x_ = transformer.transform(x)\n",
    "\n",
    "# the same code effect, but shorter\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n",
    "print(x_)\n",
    "\n",
    "# modeling\n",
    "model = LinearRegression().fit(x_, y)\n",
    "print('intercept:', model.intercept_)\n",
    "print('coefficients:', model.coef_)\n",
    "\n",
    "# Scoring\n",
    "r_sq = model.score(x_, y)\n",
    "print('coefficient of determination:', r_sq)\n",
    "\n",
    "# prediction\n",
    "y_pred = model.predict(x_)\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Mathieu Blondel\n",
    "# Jake Vanderplaas\n",
    "# License: BSD 3 clause\n",
    "# URL https://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "# function to approximate by polynomial interpolation\n",
    "def f(x):\n",
    "    return x * np.sin(x)\n",
    "\n",
    "\n",
    "# generate points used to plot\n",
    "x_plot = np.linspace(0, 10, 100)\n",
    "\n",
    "# generate points and keep a subset of them\n",
    "x = np.linspace(0, 10, 100)\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(x)\n",
    "x = np.sort(x[:20])\n",
    "y = f(x)\n",
    "\n",
    "# create matrix versions of these arrays\n",
    "X = x[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 2\n",
    "plt.plot(x_plot, \n",
    "         f(x_plot), \n",
    "         color='cornflowerblue', \n",
    "         linewidth=lw,\n",
    "         label=\"ground truth\")\n",
    "plt.scatter(x, y, \n",
    "            color='navy', \n",
    "            s=30, \n",
    "            marker='o', \n",
    "            label=\"training points\")\n",
    "\n",
    "for count, degree in enumerate([3, 4, 5]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), \n",
    "                          Ridge())\n",
    "    model.fit(X, y)\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, \n",
    "             y_plot, \n",
    "             color=colors[count], \n",
    "             linewidth=lw,\n",
    "             label=\"degree %d\" % degree)\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Linear Regression with statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "x = sm.add_constant(x)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# modeling\n",
    "model = sm.OLS(y, x)\n",
    "\n",
    "# scoring\n",
    "results = model.fit()\n",
    "print('coefficient of determination:', results.rsquared)\n",
    "print('adjusted coefficient of determination:', results.rsquared_adj)\n",
    "print('regression coefficients:', results.params)\n",
    "\n",
    "# predict\n",
    "# print('predicted response:', results.fittedvalues, sep='\\n')\n",
    "# print('predicted response:', results.predict(x), sep='\\n')\n",
    "\n",
    "# predict on new data\n",
    "x_new = sm.add_constant(np.arange(10).reshape((-1, 2)))\n",
    "print(x_new)\n",
    "y_new = results.predict(x_new)\n",
    "print(y_new)\n",
    "\n",
    "# summary\n",
    "# print(results.summary())\n",
    "# print(results.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes: Feature selection - Embedded\n",
    "\n",
    "Combine the advantages of filters and wrappers\n",
    "- The model that is being trained will directly choose the attributes that are best for it\n",
    "\n",
    "Few models support it\n",
    "* Linear models penalized by L1 (Lasso) or L1+L2 (Elastic Net) regularization: SVM, Linear regression, Logistic regression ...\n",
    "\n",
    "- Regularization introduces into the model a penalty for the number / size of model attribute weights. It's not just a prediction error. Naturally, a simpler model is chosen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
