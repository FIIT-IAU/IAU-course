{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Filter feature selection methods\n",
    "\n",
    "### 1.1 Variance Threshold\n",
    "Variance threshold is a simple baseline approach to feature selection. It removes all features whose variance doesnâ€™t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "selector = VarianceThreshold(threshold=(0.8*(1-0.8)))\n",
    "X_new = selector.fit_transform(X)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expamle: Remove columns that have a low variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from numpy import arange\n",
    "from sklearn.feature_selection import VarianceThreshold \n",
    "\n",
    "df = read_csv('../week-04/data/oil-spill.csv', header=None)\n",
    "data = df.values\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "thresholds = arange(0.0, 0.55, 0.05)\n",
    "results = list()\n",
    "for t in thresholds:\n",
    "    transform = VarianceThreshold(threshold=t)\n",
    "    X_sel = transform.fit_transform(X)\n",
    "    n_features = X_sel.shape[1]\n",
    "    print('>Threshold=%.2f, Features=%d' % (t, n_features)) \n",
    "    results.append(n_features)\n",
    "\n",
    "# plot the threshold vs the number of selected features\n",
    "pyplot.plot(thresholds, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mutual Information, Chi-Squared and F-value\n",
    "\n",
    "**Mutual information $MI$**\n",
    "\n",
    "- Mutual information $MI$ between two random variables is a non-negative value, which measures the dependency between the variables. \n",
    "- It is equal to zero if and only if two random variables are independent\n",
    "- Higher values mean higher dependency.\n",
    "\n",
    "The concept of $MI$ is linked to information theory and **information entropy** ($\\mathcal{H}$). The concept of $MI$ is linked to information theory and **information entropy** ($\\mathcal{H}$). The unit of information depends on the base of the logarithm. If the base is 2, the most used, then the information is measured in *bits*. **MI is non-negative and symmetric.**\n",
    "\n",
    "#### $\\mathcal{H}(X) = - \\int dx~\\mu(x)~log\\mu(x)$\n",
    "#### $I(X, Y) = - \\int \\int dx~dy~\\mu(x, y)~log \\frac{\\mu(x,y)}{\\mu_x(x)~\\mu_y(y)}$\n",
    "\n",
    "For discrete variables,  $H(X)$ is calculated as:\n",
    "#### $H(X) = -\\sum_i P(x_{i}) log P(x_{i})$\n",
    "\n",
    "MI can be equivalently expressed as the amount of uncertainty in $X$ minus the amount of uncertainty in $X$ after $Y$ is known, denoted as\n",
    "\n",
    "#### $I(X; Y)= H(X)- H(X|Y)$\n",
    "\n",
    "The entropy of $X$ after observing values of $Y$ is formulated by\n",
    "#### $H(X|Y) = -\\sum_{\\substack{j}} P(y_{j}) \\sum_{\\substack{i}}P(x_{i}|y_{j})\\log_2{P(x_{i}|y_{j})}$\n",
    "\n",
    "where \n",
    "- $P(x_i)$ is the prior probabilities for all values of $X$ and \n",
    "- $P(x_i|y_i)$ is the posterior probabilities of $X$ given the values of $Y$. \n",
    "\n",
    "URL https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html\n",
    "\n",
    "**Chi-Squared statistics**\n",
    "\n",
    "### $\\chi^2_{df} = \\sum \\frac{(X_i - Y_i)^2}{Y_i}$\n",
    "where:\n",
    "- $df$ = Degrees of freedom\n",
    "- $X$ = Observed value(s)\n",
    "- $Y$ = Expected value(s)\n",
    "\n",
    "URL https://www.investopedia.com/terms/c/chi-square-statistic.asp\n",
    "\n",
    "**(ANOVA) F-value**\n",
    "The correlation between each regressor and the target is computed by\n",
    "\n",
    "## $F_{value} = \\frac{variance_{dataset_1}}{variance_{dataset_2}} = \\frac{\\sigma_1^2}{\\sigma_2^2}$\n",
    "\n",
    "It is converted to an F score then to a p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape)\n",
    "\n",
    "X_new = X_new = SelectKBest(mutual_info_regression, k=2).fit_transform(X, y)\n",
    "# X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "# X_new = SelectKBest(f_regression, k=2).fit_transform(X, y)\n",
    "\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wrapper feature selection methods\n",
    "\n",
    "### 2.1 Recursive feature elimination (RFE)\n",
    "- First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a *coef_* attribute or through a *feature_importances_* attribute. \n",
    "- Then, the least important features are pruned from current set of features. \n",
    "- That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "\n",
    "estimator = SVR(kernel=\"linear\")\n",
    "selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature selection using SelectFromModel\n",
    "\n",
    "Meta-transformer for selecting features based on importance weights. The used model is usually some simple one.\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html\n",
    "\n",
    "#### Model = LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X = [[ 0.87, -1.34,  0.31 ],\n",
    "     [-2.79, -0.02, -0.85 ],\n",
    "     [-1.34, -0.48, -2.55 ],\n",
    "     [ 1.92,  1.48,  0.65 ]]\n",
    "y = [0, 1, 0, 1]\n",
    "\n",
    "selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)\n",
    "print(selector.estimator_.coef_)\n",
    "print(selector.threshold_)\n",
    "print(selector.get_support())\n",
    "\n",
    "X_new = selector.transform(X)\n",
    "print(X_new.shape)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape)\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "selector = SelectFromModel(estimator=lsvc, prefit=True)\n",
    "print(selector.get_support())\n",
    "\n",
    "X_new = selector.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
